 	Main sources of data flood
  The amount of recorded information grows by the split-second — and may be used to improve health care, change education 
  and even boost store sales.There is a huge amount of information available online. And its volume is growing at lightning speed. 
 	Each minute on average, more than 200 million emails move across the Internet (though most are spam).
 	Twitter users post more than 300,000 new tweets.
 	People across the globe share more than 38,000 Instagrams. 
 	YouTube users upload another 100 hours of video. 
 	Google processes more than 3.6 millionwebsearches. 
 	And 2.2 million things on Facebook get a “like” or a comment.
 	biologists collect enormous numbers of measurements on millions of cells and everything inside them.
 	According to the computer company IBM, 90 percent of all recorded data was created in just the last two years. 
  Most of those data are stored on computer hard drives, phones and other digital devices.
  
  Difference between data and BigData
 	 
 	Data:
 	 
 	It’s not easy to measure the total volume of data stored electronically, but an IDC estimate put the size of the “digital universe” at 4.4 zettabytes in 2013 and is forecasting a tenfold growth by 2020 to 44 zettabytes. A zettabyte is 10^21 bytes, or equivalently one thousand exabytes, one million petabytes, or one billion terabytes. That’s more than one disk drive for every person in the world.
 	The trend is for every individual’s data footprint to grow, but perhaps more significantly,the amount of data generated by machines as a part of the Internet of Things will be even greater than that generated by people. Machine logs, RFID readers, sensor networks,vehicle GPS traces, retail transactions — all of these contribute to the growing mountain of data.
 	 
 	Big Data:
 	 
 	Big data is a collection of data sets so large and complex that it becomes difficult to process using on-hand database management tools. The challenges include capture, curation, storage, search, sharing, analysis, and visualisation. The trend to larger data sets is due to the additional information derivable from analysis of a single large set of related data, as compared to separate smaller sets with the same total amount of data, allowing correlations to be found to “spot business trends, determine quality of research, prevent diseases, link legal citations, combat crime, and determine real-time roadway traffic conditions. (Wikipedia)
 	 
 	The four dimensions of Big Data
 	 
 	Volume: Large volumes of data
 	Velocity: Quickly moving data
 	Variety: structured, unstructured, images, etc.
 	Veracity: Trust and integrity is a challenge


Hadoop as a solution for data explosion
 	 
 	Apache Hadoop is 100% open source, and pioneered a fundamentally new way of storing and processing data. Instead of relying on expensive, proprietary hardware and different systems to store and process data, Hadoop enables distributed parallel processing of huge amounts of data across inexpensive, industry-standard servers that both store and process the data, and can scale without limits. With Hadoop, no data is too big. And in today’s hyperconnected world where more and more data is being created every day, Hadoop ’ s
 	breakthrough advantages mean that businesses and organizations can now find value in data that was recently considered useless.
 	But what exactly is Hadoop, and what makes it so special? In it’s basic form, it is a way of storing enormous data sets across distributed clusters of servers and then running "distributed" analysis applications in each cluster. It's designed to be robust, in that the Big
 	Data applications will continue to run even when failures occur in individual servers or clusters. It's also designed to be efficient, because it doesn't require the applications to shuttle huge volumes of data across the network. It has two main parts; a data processing
 	framework (MapReduce) and a distributed file system (HDFS) for data storage. These are the components that are at the heart of Hadoop and really make things happen.
 
